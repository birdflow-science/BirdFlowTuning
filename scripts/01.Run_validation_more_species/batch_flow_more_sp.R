##
library(BirdFlowR)
library(BirdFlowPipeline)
library(devtools)

load_all("/home/yc85_illinois_edu/BirdFlowR")
load_all("/home/yc85_illinois_edu/BirdFlowPipeline")

#######
# The `batch_flow` function used to train + validate the BirdFlow models are from BirdFlowPipeline commit ba63b3ffc8f3fa0e5757a36219d7a20ab8908ad86
# However, the BirdFlowPipeline package is now rearranged, so the batch_flow function no longer exists. To reallize the same functionaility with this updated code structure, I put a representative example at the end of this script.
# If you really want to replicate the behavior of `batch_flow`, clone the previous version of BirdFlowPipeline
# > git clone https://github.com/birdflow-science/BirdFlowPipeline.git
# > cd BirdFlowPipeline
# > git checkout ba63b3ffc8f3fa0e5757a36219d7a20ab8908ad86

unique_names <- c(
  gsub('.rds','',list.files('/work/pi_drsheldon_umass_edu/birdflow_modeling/pipeline/ground_truth_data/raw_data/motus/rds/')),
  gsub('.rds','',list.files('/work/pi_drsheldon_umass_edu/birdflow_modeling/pipeline/ground_truth_data/raw_data/banding/rds/')),
  sub("_.*", "", list.files('/work/pi_drsheldon_umass_edu/birdflow_modeling/pipeline/ground_truth_data/raw_data/tracking/', recursive = F, include.dirs=F, pattern = "\\.csv$", full.names = FALSE))
) |> unique()


#### Batch fit
for (sp in unique_names){
  print(sp)
  sp_output_path <- paste0('/project/pi_drsheldon_umass_edu/birdflow/batch_model_validation/model_output_hyperparams_distance_metric','/',sp)
  if (!dir.exists(sp_output_path)){dir.create(sp_output_path, recursive = TRUE)}

  ## Examine finished or not
  file1 <- glue::glue("/project/pi_drsheldon_umass_edu/birdflow/batch_model_validation/model_output_hyperparams_distance_metric/{sp}/{sp}_150km_interval_based_eval_using_migration_transitions/eval_metrics_train_distance_metric_all_combined.rds")
  file2 <- glue::glue("/project/pi_drsheldon_umass_edu/birdflow/batch_model_validation/model_output_hyperparams_distance_metric/{sp}/{sp}_150km_interval_based_eval_using_migration_transitions/eval_metrics_test_distance_metric_all_combined.rds")
  if (file.exists(file1) & file.exists(file2)){
    rds1 <- readRDS(file1)
    rds2 <- readRDS(file2)
    pass <- TRUE
    for (target in c('weighted_energy_improvement',
                     'weighted_mean_win_distance_fraction','synth_routes_prebreeding_migration_straightness')){
      if (!((target %in% names(rds1)) & (target %in% names(rds2)))){
        pass <- FALSE
      }
    }

    if (pass){
      print(glue::glue('Already finished for {sp}'))
      next
    } else {
      print(glue::glue('The existing files not passed for {sp}; Rerun.'))
    }
  }

  tryCatch({
    batch_flow(sp, 
               training_CV = 1,
               use_cached_data = T,
               gpu_ram = 10,
               training_n_transitions = NULL,
               hdf_path = sp_output_path,
               base_output_path = sp_output_path,
               model_selection = 'distance_metric',
               suffix='interval_based_eval_using_migration_transitions',
               # skip_quality_checks=TRUE,
               # min_season_quality = 1
               # fit_only=TRUE
               )
  }, error = function(e) {
    cat("ERROR:", conditionMessage(e), "\n")
  })
}


# 
# #### Using the newer version of BirdFlowPipeline:
# # sp = 'amewoo'
# for (resolution in c(100, 300)) {
#   
#   ## There are three functions that can be customized:
#   ## 1. loading_function (data loading function)
#   ## 2. splitting_function (data train-test split function)
#   ## 3. evaluation_function (the function to evaluate models)
#   
#   sp_output_path <- paste0(glue::glue('/project/pi_drsheldon_umass_edu/birdflow/batch_model_validation/test_resolution_{resolution}km'),'/',sp)
#   if (!dir.exists(sp_output_path)){dir.create(sp_output_path, recursive = TRUE)}
#   
#   if (
#     file.exists(file.path(sp_output_path, 'eval_res_test.rds'))
#   ) {
#     next
#   }
#   
#   ## 02. Fit batch models
#   my_batch_trainer <- BatchBirdFlowTrainer(sp,
#                                            res = resolution,
#                                            gpu_ram = 10, hdf_path = sp_output_path, base_output_path = sp_output_path,
#                                            suffix='interval_based_eval_using_migration_transitions')
#   
#   my_batch_trainer <- fit(my_batch_trainer) #force_refit=TRUE) # Fit the model with parameter grid
#   saveRDS(my_batch_trainer$params, file.path(my_batch_trainer$params$output_path, 'params.rds'))
#   # write.csv(as.data.frame(batchtools::getJobStatus()), paste0(sp_output_path, 'training_JobStatus.csv'))
#   
#   ## 03. Import transition data
#   data_loader <- TransitionsLoader(my_batch_trainer)
#   data_loader <- data_loader |> load(loading_function=get_transitions) # Here you can customize the loading_function for the transition data
#   # Save the transition data
#   saveRDS(data_loader$transitions, file.path(data_loader$batch_trainer$params$output_path, 'transitions.rds'))
#   
#   ## 04. Train test split
#   split_data <- data_loader |> 
#     split(splitting_function=train_test_split, seed=42) # Here you can customize the train_test_split function
#   
#   ## 05. Evaluate training set
#   my_evaluator <- BatchBirdFlowEvaluator(my_batch_trainer)
#   eval_res_train <- evaluate(my_evaluator, data=split_data$training_data, evaluation_function=evaluate_model)
#   
#   # Save metrics for each transition each model
#   all_transitions <- list()
#   for (this_model in eval_res_train) {
#     this_transition_df <- this_model$metric_for_each_transition
#     this_transition_df$model <- this_model$df$model
#     all_transitions[[length(all_transitions) + 1]] <- this_transition_df
#   }
#   
#   all_transitions <- do.call(rbind, all_transitions)
#   saveRDS(all_transitions, file.path(my_evaluator$batch_trainer$params$output_path, glue::glue('Training_each_transition_evaluation.rds')))
#   
#   # Get one score summary for each model
#   eval_res_train <- eval_res_train |> lapply(function(i){i$df}) |>
#     data.table::rbindlist(fill = TRUE) |>
#     tibble::as_tibble() |>
#     dplyr::arrange(dplyr::desc(.data$mean_ll))
#   saveRDS(eval_res_train, file.path(my_evaluator$batch_trainer$params$output_path, glue::glue('eval_res_train.rds')))
#   
#   ## 06. Evaluate test set
#   eval_res_test <- evaluate(my_evaluator, data=split_data$test_data, evaluation_function=evaluate_model)
#   
#   # Save metrics for each transition each model
#   all_transitions <- list()
#   for (this_model in eval_res_test) {
#     this_transition_df <- this_model$metric_for_each_transition
#     this_transition_df$model <- this_model$df$model
#     all_transitions[[length(all_transitions) + 1]] <- this_transition_df
#   }
#   
#   all_transitions <- do.call(rbind, all_transitions)
#   saveRDS(all_transitions, file.path(my_evaluator$batch_trainer$params$output_path, glue::glue('Test_each_transition_evaluation.rds')))
#   
#   # Get one score summary for each model
#   eval_res_test <- eval_res_test |> lapply(function(i){i$df}) |>
#     data.table::rbindlist(fill = TRUE) |>
#     tibble::as_tibble() |>
#     dplyr::arrange(dplyr::desc(.data$mean_ll))
#   saveRDS(eval_res_test, file.path(my_evaluator$batch_trainer$params$output_path, glue::glue('eval_res_test.rds')))
#   
#   ## 07. Get the best model and its evaluation
#   best_model <- eval_res_train |> 
#     dplyr::filter(mean_dist_cor_whole_year>0.98) |>
#     dplyr::arrange(-weighted_mean_ll) |> 
#     dplyr::slice_head(n=1) |> 
#     dplyr::select(model)
#   best_model <- best_model$model
#   
#   score_best_model <- eval_res_test[eval_res_test$model==best_model,]
#   print(c(score_best_model))
# }
# 


